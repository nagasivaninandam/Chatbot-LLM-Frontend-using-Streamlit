{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWPuHZY4URHpyIAn6mX7og",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagasivaninandam/Chatbot-LLM-Frontend-using-Streamlit/blob/master/Chatbot_LLM_Frontend_using_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be-x55DDjdtu",
        "outputId": "105349ad-6d04-4e68-f0ea-1f056248bdfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "# Colab: install Python deps\n",
        "!pip -q install streamlit openai tiktoken\n",
        "\n",
        "# Colab: install Node LocalTunnel (no account needed)\n",
        "!npm -g install localtunnel\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# --- Page setup ---\n",
        "st.set_page_config(page_title=\"Chatbot / LLM Frontend (Streamlit)\", page_icon=\"💬\", layout=\"wide\")\n",
        "\n",
        "# --- Sidebar controls ---\n",
        "st.sidebar.title(\"⚙️ Settings\")\n",
        "\n",
        "provider = st.sidebar.selectbox(\n",
        "    \"Model Provider\",\n",
        "    [\"OpenAI\", \"Dummy (no key required)\"],\n",
        "    index=0\n",
        ")\n",
        "\n",
        "openai_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\", help=\"Stored in memory for this session only.\")\n",
        "model_name = st.sidebar.text_input(\"Model name\", value=\"gpt-4o-mini\", help=\"Any chat-capable OpenAI model.\")\n",
        "\n",
        "temperature = st.sidebar.slider(\"Temperature\", 0.0, 2.0, 0.7, 0.1)\n",
        "max_tokens = st.sidebar.number_input(\"Max tokens (response)\", min_value=64, max_value=4096, value=512, step=64)\n",
        "system_prompt = st.sidebar.text_area(\n",
        "    \"System prompt\",\n",
        "    value=\"You are a helpful, concise assistant.\",\n",
        "    help=\"Sets the assistant's behavior/tone.\",\n",
        "    height=100\n",
        ")\n",
        "\n",
        "upload = st.sidebar.file_uploader(\"📎 Optional: upload a small .txt file for context\", type=[\"txt\"])\n",
        "uploaded_context = \"\"\n",
        "if upload is not None:\n",
        "    try:\n",
        "        uploaded_context = upload.read().decode(\"utf-8\", errors=\"ignore\")\n",
        "        st.sidebar.success(\"Loaded context from file.\")\n",
        "    except Exception as e:\n",
        "        st.sidebar.error(f\"Couldn't read file: {e}\")\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.caption(\"Tip: Switch to **Dummy** to test the UI without any API key.\")\n",
        "\n",
        "\n",
        "# --- Session state for messages ---\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages: List[Dict[str, Any]] = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt}\n",
        "    ]\n",
        "\n",
        "# Update system message live if user edits system_prompt\n",
        "if len(st.session_state.messages) > 0 and st.session_state.messages[0][\"role\"] == \"system\":\n",
        "    st.session_state.messages[0][\"content\"] = system_prompt\n",
        "\n",
        "# Optional context injection\n",
        "if uploaded_context:\n",
        "    # Ensure there's a single context message right after system\n",
        "    found_idx = None\n",
        "    for i, m in enumerate(st.session_state.messages):\n",
        "        if m.get(\"role\") == \"system\" and i + 1 < len(st.session_state.messages) and st.session_state.messages[i+1].get(\"role\") == \"system\" and m != st.session_state.messages[i+1]:\n",
        "            # not used, but just being careful\n",
        "            pass\n",
        "    # Replace or insert a single \"system\" context message at index 1\n",
        "    if len(st.session_state.messages) > 1 and st.session_state.messages[1].get(\"meta\") == \"context\":\n",
        "        st.session_state.messages[1][\"content\"] = f\"Extra context from user file:\\n\\n{uploaded_context}\"\n",
        "    else:\n",
        "        st.session_state.messages.insert(1, {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"Extra context from user file:\\n\\n{uploaded_context}\",\n",
        "            \"meta\": \"context\"\n",
        "        })\n",
        "\n",
        "\n",
        "# --- Header ---\n",
        "st.title(\"💬 Streamlit Chatbot / LLM Frontend\")\n",
        "st.caption(\"Supports OpenAI or a built-in Dummy mode. Try uploading a .txt file as context in the sidebar.\")\n",
        "\n",
        "\n",
        "# --- Chat history UI ---\n",
        "for m in st.session_state.messages:\n",
        "    if m[\"role\"] == \"system\":\n",
        "        # Don't render the system prompt in the main feed\n",
        "        continue\n",
        "    with st.chat_message(\"user\" if m[\"role\"] == \"user\" else \"assistant\"):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "\n",
        "# --- Helper: call OpenAI with v1 or legacy fallback ---\n",
        "def call_openai_chat(messages: List[Dict[str, str]], model: str, temperature: float, max_tokens: int, api_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Tries OpenAI Python SDK v1 (from openai import OpenAI) first;\n",
        "    falls back to legacy openai.ChatCompletion if needed.\n",
        "    \"\"\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Try v1 client\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        # Convert messages to OpenAI format (they already are)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "    except Exception as v1_err:\n",
        "        # Try legacy fallback\n",
        "        try:\n",
        "            import openai\n",
        "            openai.api_key = api_key\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "            )\n",
        "            return resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as legacy_err:\n",
        "            raise RuntimeError(f\"OpenAI call failed. v1 error: {v1_err}\\nLegacy error: {legacy_err}\")\n",
        "\n",
        "\n",
        "# --- Helper: dummy responder (no API needed) ---\n",
        "def dummy_response(user_text: str) -> str:\n",
        "    # Silly but helpful for UI testing\n",
        "    if not user_text.strip():\n",
        "        return \"Say something and I’ll reply! 🙂\"\n",
        "    return (\n",
        "        \"🤖 **Dummy mode** (no API):\\n\\n\"\n",
        "        f\"• You said: `{user_text}`\\n\"\n",
        "        f\"• Reversed: `{user_text[::-1]}`\\n\"\n",
        "        \"• Tip: Switch provider to **OpenAI** and add your key in the sidebar to get real model responses.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Chat input ---\n",
        "user_input = st.chat_input(\"Type your message\")\n",
        "if user_input is not None:\n",
        "    # Add user message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_input)\n",
        "\n",
        "    # Generate assistant reply\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            try:\n",
        "                if provider == \"OpenAI\":\n",
        "                    if not openai_key:\n",
        "                        raise ValueError(\"No OpenAI API key provided in the sidebar.\")\n",
        "                    # Prepare messages (ensure system prompt is at start)\n",
        "                    msgs = [{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in st.session_state.messages]\n",
        "                    reply = call_openai_chat(\n",
        "                        messages=msgs,\n",
        "                        model=model_name,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        api_key=openai_key,\n",
        "                    )\n",
        "                else:\n",
        "                    reply = dummy_response(user_input)\n",
        "            except Exception as e:\n",
        "                reply = f\"⚠️ Error: {e}\"\n",
        "\n",
        "        st.markdown(reply)\n",
        "\n",
        "    # Save assistant reply\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "\n",
        "# --- Footer ---\n",
        "st.markdown(\"---\")\n",
        "st.caption(\"Built with Streamlit. Dummy mode lets you test the interface without any API.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzcKA_dFj15h",
        "outputId": "d954c79f-22e3-4d78-e7b5-e5191fd811e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Streamlit (background) and then expose port 8501 with LocalTunnel\n",
        "# The LT command prints a public URL (copy it into your browser)\n",
        "!streamlit run app.py & npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfxAaK-9j5qj",
        "outputId": "2016f48c-d03d-4721-e500-5c2688edee81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.192.43.223:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0Kyour url is: https://spicy-frogs-tickle.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up anything still running ---\n",
        "!pkill -f streamlit || true\n",
        "!pkill -f node || true\n",
        "!pkill -f localtunnel || true\n",
        "!pkill -f cloudflared || true\n",
        "\n",
        "# --- Start Streamlit in the background ---\n",
        "!streamlit run app.py --server.enableCORS=false --server.enableXsrfProtection=false &>/tmp/app.log &\n",
        "\n",
        "# --- Install & run Cloudflared (no password, public URL printed below) ---\n",
        "!wget -q -O /usr/local/bin/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x /usr/local/bin/cloudflared\n",
        "!cloudflared tunnel --url http://localhost:8501 --no-autoupdate\n"
      ],
      "metadata": {
        "id": "O6UVzywok29Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start Streamlit in the background ---\n",
        "!streamlit run app.py --server.enableCORS=false --server.enableXsrfProtection=false &>/tmp/app.log &\n",
        "\n",
        "# --- Install & run Cloudflared (no password, public URL printed below) ---\n",
        "!wget -q -O /usr/local/bin/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x /usr/local/bin/cloudflared\n",
        "!cloudflared tunnel --url http://localhost:8501 --no-autoupdate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAOCnCTEmCx6",
        "outputId": "7e899e75-4ccf-42d7-9ce8-8ee0621c0543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-10-19T08:31:22Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-10-19T08:31:22Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m |  https://usual-baseball-dive-appreciated.trycloudflare.com                                 |\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.10.0 (Checksum d334c30ab628d4698455acadae5bdb3e33705a5f8dade510ce5ac7fc75d327fe)\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: ccc986f5-b4ec-4834-9e22-716ae82ab4f2\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.27\n",
            "2025/10/19 08:31:25 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-10-19T08:31:25Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0mf594dd0d-cf88-43f6-b7fe-fb3135c1e26b \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.27 \u001b[36mlocation=\u001b[0mord10 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    }
  ]
}